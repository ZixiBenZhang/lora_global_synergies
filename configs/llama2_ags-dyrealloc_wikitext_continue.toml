# basics
model = "llama_lora_ags"
dataset = "wikitext-2-v1"
load_name = "./ags_output/llama_lora_ags_causal_language_modeling_wikitext-2-v1_2024-08-01/dyrealloc_ckpts/last_chkpt-None-epoch=4-11-51.ckpt"
load_type = "pl"
backbone_model = "meta-llama/Llama-2-7b-hf"
precision = "16"
resume_training = true
# dyrealloc
importance_test_name = "grad_norm"
#metric_red_tolerance = 0.025
imp_limit_test_batches = 32
realloc_N = 0.25
turn_on_percentile = 0.33
dyrealloc_ags_mode = "combined"
# training
training_optimizer = "adamw"
learning_rate = 2e-5
weight_decay = 0.01
max_epochs = 10
batch_size = 4
log_every_n_steps = 20
# torch lightning
task = "causal_language_modeling"
num_workers = 11
num_devices = 1
#accelerator = "cpu"
strategy = "ddp_find_unused_parameters_true"
# language model options
is_pretrained = true
max_token_len = 512
project_dir = "./ags_output"
lora_config = "./configs/lora/lora-llama_networkwise_r-64.toml"
shortcut_config = "./configs/shortcut/ags_networkwise_r-64.toml"