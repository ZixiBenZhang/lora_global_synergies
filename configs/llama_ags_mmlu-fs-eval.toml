# basics
model = "llama_lora_ags"
dataset = "alpaca"
backbone_model = "meta-llama/Meta-Llama-3-8B"
load_name = "/ags_output/llama_lora_ags_causal_language_modeling_alpaca-mmlu_2024-07-28/dyrealloc_ckpts/last_chkpt-zs-epoch=1-12-49.ckpt"
load_type = "pl"
#precision = "16"
# training (useless)
training_optimizer = "adamw"
learning_rate = 1e-4
weight_decay = 0.01
max_epochs = 2
batch_size = 1
log_every_n_steps = 20
mmlu_mode = "fs"
realloc_hist_path = "/ags_output/llama_lora_ags_causal_language_modeling_alpaca-mmlu_2024-07-28/dyrealloc_ckpts/reallocation_history_grad-norm_12-49-58.toml"
# torch lightning
task = "causal_language_modeling"
num_workers = 11
num_devices = 1
#accelerator = "cpu"
strategy = "ddp_find_unused_parameters_true"
# language model options
is_pretrained = true
max_token_len = 4096
project_dir = "./ags_output"
lora_config = "./configs/lora/lora-llama_networkwise.toml"
shortcut_config = "./configs/shortcut/ags_networkwise.toml"
