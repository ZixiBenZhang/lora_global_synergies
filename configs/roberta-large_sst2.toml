# basics
model = "roberta-large"
dataset = "sst2"
# training
training_optimizer = "adamw"
#learning_rate = 1e-4
weight_decay = 0.01
max_epochs = 5
batch_size = 8
accumulate_grad_batches = 1
log_every_n_steps = 20
# torch lightning
task = "classification"
#num_workers = 0
num_devices = 4
#accelerator = "cpu"
# language model options
is_pretrained = true
project_dir = "./ags_output"