# basics
model = "gemma_lora"
dataset = "qqp"
backbone_model = "google/gemma-2b"
# training
training_optimizer = "adamw"
learning_rate = 1e-4
weight_decay = 0.01
max_epochs = 10
batch_size = 4
log_every_n_steps = 20
# torch lightning
task = "classification"
num_workers = 42
num_devices = 2
#accelerator = "cpu"
# language model options
is_pretrained = true
project_dir = "./ags_output"
lora_config = "./configs/lora/lora_networkwise.toml"
load_name = "./ags_output/gemma_lora_classification_qqp_2024-04-23/training_ckpts/last_chkpt.ckpt"
load_type = "pl"
resume_training = true