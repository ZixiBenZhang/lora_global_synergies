# Finetune LoRA modules with config dynamically reallocated based on importance testing
# basics
model = "opt_lora"
dataset = "mrpc"
load_name = "facebook/opt-350m"
load_type = "hf"
# reallocation
importance_test_name = "constant"
metric_red_tolerance = 0.005
imp_limit_test_batches = 32
realloc_N = 1.0
turn_on_percentile = 0.25
# training
training_optimizer = "adamw"
learning_rate = 3e-4
weight_decay = 0.01
max_epochs = 20
batch_size = 4
log_every_n_steps = 10
# torch lightning
task = "classification"
#num_workers = 0
num_devices = 2
#accelerator = "cpu"
strategy = "ddp_find_unused_parameters_true"
# language model options
is_pretrained = true
project_dir = "./ags_output"
lora_config = "./configs/lora/lora_networkwise.toml"